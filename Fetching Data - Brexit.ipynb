{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data: GetOldTweets3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important dates for brexit process:\n",
    "\n",
    "a) 23 June 2016 – Referendum out to leave EU<br>\n",
    "b) 29 March 2017 – Prime Minister formally triggered Article 50 and began the two-year countdown to the UK formally leaving the EU <br>\n",
    "c) 14 March 2019 – Government sought permission from the EU to extend Article 50 and agree a later Brexit date<br>\n",
    "d)\t10 April 2019 – UK and EU27 agreed to extend Article 50 until 31 October 2019<br>\n",
    "e)\t19 October 2019 – Prime Minister’s new Brexit deal was lost on amendment in the Commons. Requests further extension to Brexit process<br>\n",
    "f)\t28 October 2019 – EU Ambassadors agreed a further Brexit extension to 31 January 2020<br>\n",
    "g)  13 December 2019 - UK Elections: Landslide victory for Conservative, Boris Johnson to lead Brexit<br>\n",
    "\n",
    "<b><i>Break down data from a) into: </b></i><br>\n",
    "a11, a12: 1 month prior (May 2016) <br>\n",
    "a21, a22: the month of Brexit Referendum (June 2016) <br>\n",
    "a31, a32: 1 month after (July 2016) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install GetOldTweets3\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_dataset(query_search, since_date, until_date, max_tweets, lang, username = None):\n",
    "    import pandas as pd\n",
    "    import GetOldTweets3 as got\n",
    "    tweets = []\n",
    "    for word in query_search:\n",
    "        if username != None:\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word)\\\n",
    "                                                    .setUsername(username)\\\n",
    "                                                    .setSince(since_date)\\\n",
    "                                                    .setUntil(until_date)\\\n",
    "                                                    .setMaxTweets(max_tweets)\\\n",
    "                                                    .setLang(lang)\\\n",
    "                                                    .setEmoji(\"unicode\")\n",
    "                                                \n",
    "        else:\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word)\\\n",
    "                                                        .setSince(since_date)\\\n",
    "                                                        .setUntil(until_date)\\\n",
    "                                                        .setMaxTweets(max_tweets)\\\n",
    "                                                        .setLang(lang)\\\n",
    "                                                        .setEmoji(\"unicode\")\n",
    "        for tweet in got.manager.TweetManager.getTweets(tweetCriteria):\n",
    "            if not tweet in tweets:\n",
    "                tweets.append(tweet)\n",
    "    hashs = []\n",
    "    texts = []\n",
    "    dates = []\n",
    "    retweets = []\n",
    "    favorites = []\n",
    "    usernames = []\n",
    "    location = []\n",
    "    for tweet in tweets:\n",
    "        hash_positions = [pos for pos, char in enumerate(tweet.hashtags) if char == '#']\n",
    "        tweet_hashs = []\n",
    "        texts.append(tweet.text)\n",
    "        dates.append(tweet.date)\n",
    "        retweets.append(tweet.retweets)\n",
    "        favorites.append(tweet.favorites)\n",
    "        usernames.append(tweet.username)\n",
    "        location.append(tweet.geo)\n",
    "        for ind in hash_positions:\n",
    "            if tweet.hashtags.find(' ', ind) == -1:\n",
    "                word = tweet.hashtags[ind:]\n",
    "                tweet_hashs.append(word)\n",
    "            else:\n",
    "                end = tweet.hashtags.find(' ', ind)\n",
    "                word = tweet.hashtags[ind:end]\n",
    "                tweet_hashs.append(word)\n",
    "        hashs.append(tweet_hashs)\n",
    "    return tweets, pd.DataFrame({\"Username\":usernames, \"Date\":dates, \"Favorites\":favorites, \"Retweets\":retweets, \"Hashtags\":hashs, \"Text\":texts, \"Location\":location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) 1-week range around June 23, 2016\n",
    "Big Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [f'2016-05-0{x}' for x in range(1,10)] + [f'2016-05-{x}' for x in range(10,17)] # 2016-05-01 to 2016-05-16\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [f'2016-05-{x}' for x in range(16,32)] # 2016-05-16 to 2016-05-31\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [f'2016-06-0{x}' for x in range(1,10)] + [f'2016-06-{x}' for x in range(10,17)] # 2016-06-01 to 2016-06-16\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [f'2016-06-{x}' for x in range(16,31)] # 2016-06-16 to 2016-06-30\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done 4538\n",
      "1 done 4612\n",
      "2 done 4335\n",
      "3 done 4421\n",
      "4 done 4174\n",
      "5 done 3916\n",
      "6 done 3862\n",
      "7 done 3326\n",
      "8 done 3212\n",
      "9 done 3603\n",
      "10 done 3919\n",
      "11 done 3202\n",
      "12 done 3078\n",
      "13 done 3085\n",
      "14 done 2733\n",
      "56016\n"
     ]
    }
   ],
   "source": [
    "dates = [f'2016-07-0{x}' for x in range(1,10)] + [f'2016-07-{x}' for x in range(10,17)] # 2016-07-01 to 2016-07-16\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done 2176\n",
      "1 done 2479\n",
      "2 done 2842\n",
      "3 done 2475\n",
      "4 done 2968\n",
      "5 done 3008\n",
      "6 done 2486\n",
      "7 done 1985\n",
      "8 done 2190\n",
      "9 done 2572\n",
      "10 done 2049\n",
      "11 done 2280\n",
      "12 done 1986\n",
      "13 done 2037\n",
      "14 done 1700\n",
      "35233\n"
     ]
    }
   ],
   "source": [
    "dates = [f'2016-07-{x}' for x in range(16,32)] # 2016-07-16 to 2016-07-31\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_a32.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) 1-week range after March 29, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2017-03-29','2017-03-30','2017-03-31','2017-04-01','2017-04-02','2017-04-03','2017-04-04','2017-04-05']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_b.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) 1-week range after March 14, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-03-14','2019-03-15','2019-03-16','2019-03-17','2019-03-18','2019-03-19','2019-03-20','2019-03-21']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) 1-week range after October 4, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-04-10','2019-04-11','2019-04-12','2019-04-13','2019-04-14','2019-04-15','2019-04-16','2019-04-17']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) 1-week range after October 19, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-10-19','2019-10-20','2019-10-21','2019-10-22','2019-10-23','2019-10-24','2019-10-25','2019-10-26']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_e.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) 1-week range after October 28, 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-10-28','2019-10-29','2019-10-30','2019-10-31','2019-11-01','2019-11-02','2019-11-03','2019-11-04']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "# a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_f.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g) 1-week range around December 13, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-12-10', '2019-12-11', '2019-12-12', '2019-12-13' , '2019-12-14', '2019-12-15', '2019-12-16']\n",
    "data = np.zeros((0,0))\n",
    "a_dataset = pd.DataFrame(data)\n",
    "\n",
    "hashtags = ['brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'notoeu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin', 'yes2eu', 'yestoeu', 'intogether', 'vote_remain']\n",
    "\n",
    "for i in range(len(dates)-1):\n",
    "    d1, d2 = get_tweets_dataset(hashtags,dates[i],dates[i+1], 500, 'en')\n",
    "    a_dataset = a_dataset.append(d2, ignore_index=True)\n",
    "    print(f'{i} done', len(d2))\n",
    "\n",
    "print(len(a_dataset))\n",
    "a_dataset.to_csv('C:/Users/tho_j/Documents/Columbia U/IEOR E4523 Data Analytics/Brexit/data2/data_g.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data: Twitter Search API\n",
    "\n",
    "We used this before the presentation to find data for period f. However, due to the daily limits on live tweets that can be streamed, we decided to go with data obtained from GetOldTweets3 for period f when we were preparing this report. Below is the code we used when we first utilized the Twitter Search API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy\n",
    "!pip install forex_python\n",
    "import sys\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweet_list = list() #contains unique tweets\n",
    "unique_id_list = list() #contains unique user_ids\n",
    "\n",
    "#initialise twitter API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#keyword to search\n",
    "a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q= 'brexit','LeaveEUOfficial', 'LeaveEU', 'LabourLeave', 'ukip', 'no2eu', 'britainout', 'leaveeu', 'voteleave', 'ukineu', 'VoteRemain', 'LabourInForBritain', 'euref', 'eureferendum', 'betteroffout', 'eureform', 'betteroffin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(unique_tweet_list, sinceid, date):\n",
    "    results = api.search(q = a or b or c or d or e or f or g or h or i or j or k or l or m or n,\\\n",
    "                         lang = \"en\", result_type = \"recent\", count = 100, since_id = sinceid,\\\n",
    "                         until = date, tweet_mode = 'extended')\n",
    "    for item in results:\n",
    "        if item._json not in unique_tweet_list:\n",
    "            unique_tweet_list.append(item._json)\n",
    "       \n",
    "        if item._json['user']['id'] not in new_id_list:\n",
    "            new_id_list.append(item._json['user']['id'])\n",
    "\n",
    "    return unique_tweet_list, new_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch the tweets for id not in unique id such that tweets are not duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_more():\n",
    "    new_id_list = list()\n",
    "    dates = ['2019-11-30','2019-12-01','2019-12-02','2019-12-03','2019-12-04','2019-12-05']\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(unique_id_list),10):\n",
    "            sinceid = unique_id_list[i] #change ids at an interval of 30\n",
    "            for date in dates:\n",
    "                unique_tweet_list, new_id_list = get_tweets(unique_tweet_list, sinceid, date)\n",
    "\n",
    "                print('len of tweets:',len(unique_tweet_list))\n",
    "                print('len of ids:',len(new_id_list))\n",
    "    except:\n",
    "        print('Daily limit of fetching tweets exceeded! Please wait for 24 hours to start fetching again. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the unique id list with the newly fetched data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    temp = []\n",
    "    for id_ in new_id_list:\n",
    "        if id_ not in unique_id_list:\n",
    "            temp.append(id_)\n",
    "\n",
    "    unique_id_list = temp\n",
    "    new_id_list = []\n",
    "    print(len(unique_id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the tweet text, location, date, language, retweet count and favorite count from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tweets = []\n",
    "for tweet in unique_tweet_list:\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        tweets.append(tweet['retweeted_status']['full_text'])\n",
    "    else:\n",
    "        tweets.append(tweet['full_text'])\n",
    "\n",
    "retweetcount = [x['retweet_count'] for x in unique_tweet_list]\n",
    "favoritecount = [x['favorite_count'] for x in unique_tweet_list]\n",
    "createddate = [x['created_at'] for x in unique_tweet_list]\n",
    "language = [x['metadata']['iso_language_code'] for x in unique_tweet_list]\n",
    "location = [x['user']['location'] for x in unique_tweet_list]\n",
    "\n",
    "\n",
    "col = ['tweets']\n",
    "df2 = pd.DataFrame(tweets, columns = col)\n",
    "df2['createddate'] = createddate\n",
    "df2['location'] = location\n",
    "df2['language'] = language\n",
    "df2['retweetcount'] = retweetcount\n",
    "df2['favoritecount']= favoritecount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the duplicates if any from the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
